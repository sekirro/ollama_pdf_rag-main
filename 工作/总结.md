## RAG过程

### 1. 读入文件，转换为Document类型

不同的文件类型使用`langchain_community.document_loaders`中的不同读入方式

```python
supported_extensions = {
    '.pdf': PyPDFLoader,
    '.docx': Docx2txtLoader,
    '.csv': CSVLoader,
    '.xlsx': UnstructuredExcelLoader,
    '.txt': TextLoader,
}
```

代码文件使用专门的代码读入库进行读入，由于文本分割需要`document`类型的数据，因此还需要转化为`document`类型

```python
Document(
    page_content=formatted_content,
    metadata={
        'source': doc['path'],
        'filename': doc['filename'],
        'extension': doc['extension']
    }
)
```

读取代码文件内容函数：

```python
## 代码文件
def extract_code(file_path, file_extension):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    # Java代码解析
    if file_extension == '.java':
        try:
            tree = javalang.parse.parse(content)
            # 提取类名、方法名、变量名等
            analysis = []
            for path, node in tree.filter(javalang.tree.ClassDeclaration):
                analysis.append(f"类名: {node.name}")
                for method in node.methods:
                    analysis.append(f"方法: {method.name}")
                    if method.documentation:
                        analysis.append(f"文档: {method.documentation}")
            return "\n".join(analysis) + "\n原始代码:\n" + content
        except:
            return content

    # Python代码解析
    elif file_extension == '.py':
        try:
            tree = ast.parse(content)
            analysis = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    analysis.append(f"类名: {node.name}")
                elif isinstance(node, ast.FunctionDef):
                    analysis.append(f"函数: {node.name}")
                    if ast.get_docstring(node):
                        analysis.append(f"文档: {ast.get_docstring(node)}")
            return "\n".join(analysis) + "\n原始代码:\n" + content
        except:
            return content

    # C/C++代码解析
    elif file_extension in ['.cpp', '.c', '.h', '.hpp']:
        try:
            index = clang.cindex.Index.create()
            tu = index.parse(file_path)
            analysis = []

            def process_node(node):
                if node.kind == clang.cindex.CursorKind.FUNCTION_DECL:
                    analysis.append(f"函数: {node.spelling}")
                elif node.kind == clang.cindex.CursorKind.CLASS_DECL:
                    analysis.append(f"类名: {node.spelling}")
                for child in node.get_children():
                    process_node(child)

            process_node(tu.cursor)
            return "\n".join(analysis) + "\n原始代码:\n" + content
        except:
            return content

    # JavaScript代码解析
    elif file_extension == '.js':
        try:
            ast = esprima.parseScript(content)
            analysis = []

            def process_node(node):
                if node.type == 'FunctionDeclaration':
                    analysis.append(f"函数: {node.id.name}")
                elif node.type == 'ClassDeclaration':
                    analysis.append(f"类名: {node.id.name}")

            for node in ast.body:
                process_node(node)
            return "\n".join(analysis) + "\n原始代码:\n" + content
        except:
            return content
```

转换为documents类型函数

```python
def transform_documents(folder_path):
    # 处理文件夹中的所有文件
    file_contents = process_folder(folder_path)
    
    # 创建Document对象列表
    # 向量化时不会对metadata向量化
    documents = []
    for doc in file_contents:
        formatted_content = f"""
        文件名: {doc['filename']}
        文件路径: {doc['path']}
        文件类型: {doc['extension']}
        ---
        {doc['content']}
        """
        # 创建Document对象
        documents.append(
            Document(
                page_content=formatted_content,
                metadata={
                    'source': doc['path'],
                    'filename': doc['filename'],
                    'extension': doc['extension']
                }
            )
        )
    return documents
```

### 2. 文本分割

使用文本分割器，将数据进行分割。

将列表中`Document`类型数据分割为更小的块，每一个块还是`Document`类型，`page_content`为分割后的内容，`metadata`继承源文档的`metadata`

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
chunks = text_splitter.split_documents(data)
```

**通用参数**

```python
# 通用参数
{
    "chunk_size": "块的最大大小（字符数或token数）",
    "chunk_overlap": "相邻块之间的重叠部分大小",
    "length_function": "计算文本长度的函数",
    "add_start_index": "是否在元数据中添加起始索引",
    "strip_whitespace": "是否删除空白字符",
    "separators": "分隔符列表",
    "keep_separator": "是否保留分隔符"
}

# 特定分割器的参数
{
    "encoding_name": "TokenTextSplitter的编码方式",
    "model_name": "使用的模型名称",
    "pipeline": "Spacy模型名称",
    "tags_to_split_on": "HTML标签列表",
    "headers_to_split_on": "Markdown标题格式"
}
```

**不同文本分割器**

1. **RecursiveCharacterTextSplitter（递归字符文本分割器）**

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,          # 每个块的最大字符数
    chunk_overlap=200,        # 块之间重叠的字符数
    length_function=len,      # 计算长度的函数
    separators=["\n\n", "\n", " ", "，", ".", "。"],  # 分割符号，按优先级排序
    keep_separator=True,      # 是否保留分隔符
    add_start_index=False,    # 是否在元数据中添加起始索引
    strip_whitespace=True     # 是否删除空白字符
)
## 尝试不同的分割符号，若chunks过大，尝试下一个分割符号
```

2. **CharacterTextSplitter（字符文本分割器）**
```python
## 只使用一个分割符号，不如RecursiveCharacterTextSplitter（递归字符文本分割器）
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(
    separator="\n\n",         # 分割符号
    chunk_size=1000,         
    chunk_overlap=200,
    length_function=len,
    keep_separator=True,
    add_start_index=False
)
```

3. **TokenTextSplitter**
```python
## 按照token数分割
from langchain_text_splitters import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=100,           # token数量而不是字符数
    chunk_overlap=20,
    encoding_name="cl100k_base",  # tokenizer编码
    model_name="gpt-4",          # 使用的模型名称
    add_start_index=False
)
```

4. **MarkdownHeaderTextSplitter（Markdown标题分割器）**
```python
from langchain_text_splitters import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)
```

5. **PythonCodeTextSplitter（Python代码分割器）**
```python
from langchain_text_splitters import PythonCodeTextSplitter

splitter = PythonCodeTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
```

6. **HTMLTextSplitter（HTML文本分割器）**
```python
from langchain_text_splitters import HTMLTextSplitter

splitter = HTMLTextSplitter(
    tags_to_split_on=["div", "p"],  # HTML标签
    chunk_size=1000,
    chunk_overlap=200
)
```

代码语言分割器：

```python
from langchain_text_splitters import Language

# 查看所有支持的语言
print(Language.__members__)

# 主要包括：
{
    'CPP',         # C++
    'GO',          # Go
    'JAVA',        # Java
    'JS',          # JavaScript
    'PHP',         # PHP
    'PROTO',       # Protocol Buffers
    'PYTHON',      # Python
    'RST',         # reStructuredText
    'RUBY',        # Ruby
    'RUST',        # Rust
    'SCALA',       # Scala
    'SWIFT',       # Swift
    'MARKDOWN',    # Markdown
    'LATEX',       # LaTeX
    'HTML',        # HTML
    'SOL',         # Solidity
    'TYPESCRIPT'   # TypeScript
}
```

7. **SpacyTextSplitter（基于Spacy的分割器）**

```python
## 基于自然语言的分割器
from langchain_text_splitters import SpacyTextSplitter

splitter = SpacyTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    pipeline="zh_core_web_sm"  # 使用的Spacy模型
)
```

8. **NLTKTextSplitter（基于NLTK的分割器）**
```python
## 基于自然语言的分割器
from langchain_text_splitters import NLTKTextSplitter

splitter = NLTKTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
```

### 3. 文本向量化

选择嵌入式模型

```python
# 不同的 Embedding 选择
from langchain_openai import OpenAIEmbeddings  # OpenAI
from langchain_ollama import OllamaEmbeddings  # Ollama
from langchain_community.embeddings import (
    HuggingFaceEmbeddings,     # HuggingFace
    CohereEmbeddings,          # Cohere
    SentenceTransformerEmbeddings  # Sentence Transformers
)
```

### 4. 向量存储

```python
from langchain_community.vectorstores import (
    Chroma,          # ChromaDB
    FAISS,           # Facebook AI Similarity Search
    Pinecone,        # Pinecone
    Milvus,          # Milvus
    Qdrant           # Qdrant
)
vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=OllamaEmbeddings(model="nomic-embed-text"),
    collection_name="local-rag"
)
```

### 5. 检索器设置

#### 基础

1. **基础检索器 (Basic Retriever)**
```python
from langchain_community.vectorstores import Chroma

# 最简单的向量检索器
basic_retriever = vector_store.as_retriever(
    search_type="similarity",  # 相似度搜索
    search_kwargs={
        "k": 4,                # 返回文档数量
        "score_threshold": 0.5  # 相似度阈值
    }
)
# 1. 相似度搜索 (similarity)
similarity_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 4,
        "score_threshold": 0.5
    }
)

# 2. MMR搜索 (mmr)
# 直接相似度搜索可能返回内容高度相似的文档
mmr_retriever = vector_store.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance
    search_kwargs={
        "k": 4,                    # 返回数量
        "fetch_k": 20,             # 候选数量
        "lambda_mult": 0.5         # 多样性权重
    }
)

# 3. 相似度分数阈值搜索 (similarity_score_threshold)
# 严格相似度要求，返回0-k个，避免不相关的信息
threshold_retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,    # 分数阈值
        "k": 4                     # 最大返回数量
    }
)
```

2. **时间加权检索器 (TimeWeightedVectorStoreRetriever)**

```python
from langchain.retrievers import TimeWeightedVectorStoreRetriever

# 考虑文档时间因素的检索器，适合新闻、社交媒体等时效性内容
# 需要获取文档的时间戳
time_retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vector_store,
    decay_rate=0.01,
    k=4
)
```

3. **自查询检索器 (SelfQueryRetriever)**

```python
# LLM用于将自然语言查询转换为结构化查询
from langchain.retrievers import SelfQueryRetriever

# 示例：用户输入自然语言查询
natural_query = "找出张三在2023年写的关于Python的文档"

# LLM会将其转换为结构化查询
structured_query = {
    "query_text": "Python文档",  # 用于向量搜索的文本
    "filter": {                  # 元数据过滤条件
        "author": "张三",
        "date": {"$gte": "2023-01-01", "$lte": "2023-12-31"}
    }
}
```

```python
from langchain.retrievers import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo
# 能够理解查询意图并自动构建查询条件
# document_contents是对文档集合的描述，帮助LLM理解上下文
# metadata_field_info定义了可用于过滤的元数据字段
# metadata_field_info定义了可用于过滤的元数据字段
metadata_field_info = [
    AttributeInfo(
        name="author",              # 字段名
        description="文档作者",      # 字段描述
        type="string"              # 字段类型
    ),
    AttributeInfo(
        name="date",
        description="文档创建日期，格式：YYYY-MM-DD",
        type="date"
    ),
    AttributeInfo(
        name="tags",
        description="文档标签，如：Python, Java, ML等",
        type="string"
    ),
]

# 这些信息帮助LLM：
# 1. 知道有哪些可用的过滤字段
# 2. 理解每个字段的含义和类型
# 3. 正确构造过滤条件
self_query_retriever = SelfQueryRetriever.from_llm(
    llm,
    vector_store,
    document_contents="文档描述",
    metadata_field_info=metadata_field_info
)
```

4. **父文档检索器 (ParentDocumentRetriever)**

```python
# 1. 向量存储选项
from langchain.vectorstores import (
    Chroma,     # 本地向量存储
    FAISS,      # Facebook AI相似性搜索
    Pinecone,   # 云端向量数据库
    Milvus      # 分布式向量数据库
)

# 2. 文档存储选项
from langchain.storage import (
    InMemoryStore,   # 内存存储
    LocalFileStore,  # 本地文件存储
    RedisStore,      # Redis存储
    S3Store          # AWS S3存储
)
```

```python
from langchain.retrievers import ParentDocumentRetriever

# 不需要提前进行文档分割，在添加文档时child_splitter和parent_splitter自动进行分割
# 检索时进行子文档的检索（向量），返回的是完整的父文档
# 解决上下文缺失的问题
vectorstore = Chroma(
    embedding_function=self.embeddings,
    persist_directory="chroma_db"
)
docstore = InMemoryStore()
parent_retriever = ParentDocumentRetriever(
    vectorstore=vector_store,# vectorstore:存储子文档的向量表示
    docstore=docstore,# docstore:存储完整的父文档
    child_splitter=text_splitter,
    parent_splitter=parent_splitter
)

documents = []
parent_retriever.add_documents(documents)
```

#### 高级

5. **多查询检索器 (MultiQueryRetriever)**

```python
from langchain.retrievers import MultiQueryRetriever

# 通过LLM生成多个查询变体来提高召回率
multi_retriever = MultiQueryRetriever.from_llm(
    retriever=vector_store.as_retriever(),
    llm=llm,
    prompt=QUERY_PROMPT,  # 自定义提示模板
    num_queries=3        # 生成查询变体数量
)
```

6. **基于关键词的检索 (Keyword Search)**

```python
from langchain.retrievers import (
    BM25Retriever,          # BM25算法检索器
    TFIDFRetriever,         # TF-IDF算法检索器
    SVMRetriever,           # SVM检索器
    KeywordTableRetriever   # 关键词表检索器
)

# TF-IDF 检索器
tfidf_retriever = TFIDFRetriever.from_documents(
    documents,
    k=4  # 返回前k个结果
)

# BM25 检索器
bm25_retriever = BM25Retriever.from_documents(
    documents,
    k=4
)
```

7. **重排序检索器 (ReRankerRetriever)**

```python
## 对基检索器的检索结果二次排序，提高检索质量，可自定义排序规则
from langchain.retrievers import ReRankerRetriever
from langchain.retrievers.document_compressors import CohereRerank

# 创建基础检索器
base_retriever = vector_store.as_retriever(
    search_kwargs={"k": 10}  # 初始检索更多文档
)

# 创建 Cohere 重排序器
cohere_reranker = CohereRerank(
    model="rerank-multilingual-v2.0",  # 多语言模型
    top_n=5,                          # 返回前N个结果
    api_key="your-api-key"            # Cohere API密钥
)

# 使用重排序模型优化检索结果
reranker_retriever = ReRankerRetriever(
    base_retriever=vector_store.as_retriever(),
    reranker=cohere_reranker  # 或其他重排序模型
)
```

```python
# 自定义
from sentence_transformers import CrossEncoder

class CustomReranker:
    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model = CrossEncoder(model_name)
    
    def rerank(self, query: str, documents: list, top_n: int = 5):
        # 准备文档对
        pairs = [[query, doc.page_content] for doc in documents]
        
        # 计算相关性分数
        scores = self.model.predict(pairs)
        
        # 排序并返回前 top_n 个文档
        scored_docs = list(zip(documents, scores))
        ranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in ranked_docs[:top_n]]

# 使用自定义重排序器
custom_reranker = CustomReranker()
custom_reranker_retriever = ReRankerRetriever(
    retriever=base_retriever,
    reranker=custom_reranker,
    num_retrieved=10,
    num_reranked=5
)
```

8. **合并检索器 (EnsembleRetriever)**

```python
from langchain.retrievers import EnsembleRetriever

# 组合多个检索器
ensemble_retriever = EnsembleRetriever(
    retrievers=[basic_retriever, multi_retriever],
    weights=[0.5, 0.5]
)
```

### 6. RAG

```python
# llm
local_model = "deepseek-r1"
llm = ChatOllama(model=local_model)

# retriver
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""你是一个 AI 语言模型助手。你的任务是为用户的问题生成五个不同的变体版本，
    用于从向量数据库中检索相关文档。通过生成多个不同角度的问题，你的目标是帮助用户克服
    基于距离相似度搜索的一些局限性。请提供这些替代性问题，每个问题用换行符分隔。
    原始问题: {question}""",
)
retriever = MultiQueryRetriever.from_llm(
    vector_db.as_retriever(), 
    llm,
    prompt=QUERY_PROMPT
)

# prompt
template = """基于以下内容回答问题:
{context}
问题: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
s = chain.invoke("我的PPO是如何实现的?帮我完善一下我的代码，给出完整的")
```

一种信息过滤

## 后续工作

1. 大模型能力问题：

是否是因为参数不够，是否是因为能力问题。可以尝试反馈机制，让大模型先生成一个基础代码

2. 检索方面：

优化检索，代码转化为向量的问题



## 代码与文本向量距离问题

嵌入式模型的性能很重要，`OllamaEmbeddings(model="nomic-embed-text")`这一嵌入式模型应该是适用于英文，`BAAI/bge-large-zh-v1.5`则适用于中文语境

可以看HuggingFace上的mteb排行

1. 将文件名/文件路径与文件内容作为数据一同进行向量嵌入

   ```python
   Document(
       page_content=formatted_content,
       metadata={
           'source': doc['path'],
           'filename': doc['filename'],
           'extension': doc['extension']
       }
   )
   ```

2. 尝试不同的向量嵌入模型

- ollama的不同嵌入模型
- 使用专门代码向量嵌入模型`microsoft/codebert-base`（需要显卡）

3. 尝试不同的向量存储

- 使用`langchain_community.vectorstores`的`Chroma`进行向量存储
- 使用`langchain_community.vectorstores`的`FAISS`进行向量存储
- 使用`langchain_community.vectorstores`的`Pinecone`进行向量存储
- 使用`langchain_community.vectorstores`的`Zilliz`进行向量存储

4. 尝试不同的检索器

- 使用`langchain.retrievers.multi_query`的`MultiQueryRetriever`进行检索
- 使用`langchain.retrievers.contextual_compression`的`ContextualCompressionRetriever`进行检索
- 使用`langchain.retrievers.contextual_compression`的`SelfQueryRetriever`进行检索
- 使用`langchain.retrievers.contextual_compression`的`RAG`进行检索
