{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from IPython.display import display as Markdown\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "053421d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredExcelLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    documents = []\n",
    "    supported_extensions = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '.csv': CSVLoader,\n",
    "        '.xlsx': UnstructuredExcelLoader,\n",
    "        '.txt': TextLoader,\n",
    "        # '.md': TextLoader,\n",
    "        # '.markdown': TextLoader\n",
    "    }\n",
    "    \n",
    "    # 为代码文件单独处理\n",
    "    code_extensions = {'.py', '.java', '.js', '.cpp', '.c', '.h', '.hpp', '.md', '.markdown'}\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_extension = os.path.splitext(file)[1].lower()\n",
    "            \n",
    "            try:\n",
    "                if file_extension in supported_extensions:\n",
    "                    # 对于支持的文件类型，先检测编码\n",
    "                    if file_extension in {'.txt', '.csv'}:  # 只对文本类型的文件检测编码\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            raw_data = f.read()\n",
    "                            detected = chardet.detect(raw_data)\n",
    "                            encoding = detected['encoding']\n",
    "                        \n",
    "                        loader_class = supported_extensions[file_extension]\n",
    "                        if file_extension == '.csv':\n",
    "                            loader = loader_class(file_path, encoding=encoding)\n",
    "                        else:\n",
    "                            loader = loader_class(file_path, encoding=encoding)\n",
    "                    else:\n",
    "                        # 对于非文本类型的文件（如pdf, docx等）使用默认加载器\n",
    "                        loader_class = supported_extensions[file_extension]\n",
    "                        loader = loader_class(file_path)\n",
    "                    docs = loader.load()\n",
    "                    \n",
    "                elif file_extension in code_extensions:\n",
    "                    # 对代码文件特殊处理\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        raw_data = f.read()\n",
    "                        detected = chardet.detect(raw_data)\n",
    "                        encoding = detected['encoding']\n",
    "                    loader = TextLoader(file_path, encoding=encoding)\n",
    "                    docs = loader.load()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # 添加额外的元数据\n",
    "                for doc in docs:\n",
    "                    file_path = os.path.relpath(file_path, folder_path)\n",
    "                    formatted_content = f\"\"\"\n",
    "文件名: {file}\n",
    "文件路径: {file_path}\n",
    "文件类型: {file_extension}\n",
    "---\n",
    "{doc.page_content}\n",
    "\"\"\"\n",
    "                    doc.page_content = formatted_content\n",
    "                    doc.metadata.update({\n",
    "                        'filename': file,\n",
    "                        'extension': file_extension,\n",
    "                        'relative_path': file_path\n",
    "                    })\n",
    "                documents.extend(docs)\n",
    "                print(f\"成功处理文件: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# 使用修改后的函数\n",
    "def transform_documents(folder_path):\n",
    "    # 直接返回处理后的文档列表\n",
    "    return process_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "edfd1176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功处理文件: CartPole_introduction.md\n",
      "成功处理文件: p_iteration.py\n",
      "成功处理文件: QLearning.py\n",
      "成功处理文件: Sarsa.py\n",
      "成功处理文件: v_iteration.py\n"
     ]
    }
   ],
   "source": [
    "local_path = \"D:/Desktop/ollama_pdf_rag-main/documents_for_analyse\"\n",
    "data = transform_documents(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "600c24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文件名: CartPole_introduction.md\n",
      "文件路径: CartPole_introduction.md\n",
      "文件类型: .md\n",
      "---\n",
      "# Cartpole 介绍文档\n",
      "\n",
      "## 什么是 Cartpole\n",
      "\n",
      "Cartpole 是一个经典的强化学习问题，也被称为倒立摆问题。它通常用作强化学习算法的基准测试环境。该问题的基本目标是让一个杆子保持垂直平衡，避免倒下。系统由一个带有固定轨道的小车和一根连接到小车的杆子组成。通过控制小车的水平运动，可以影响杆子的角度，并试图让杆子保持垂直位置。\n",
      "\n",
      "![动图封面](https://pic3.zhimg.com/v2-44077b4784f621dcf4e3556a455f9f32_b.jpg)\n",
      "\n",
      "## 问题描述\n",
      "\n",
      "Cartpole 环境的基本组成包括：\n",
      "\n",
      "- **小车**：一个可以在水平面上前后移动的物体。\n",
      "- **杆子**：一个垂直于小车的杆子，杆子上端与小车相连接。\n",
      "- **控制输入**：通过向小车施加力（可以是向左或向右），控制小车的水平位置和速度。\n",
      "\n",
      "系统的目标是通过施加适当的力来防止杆子倒下。每当杆子的角度偏离垂直位置超过某个阈值，或者小车移出轨道，任务就被认为失败。为了保持杆子平衡，小车需要不断调整其位置。\n",
      "\n",
      "### 环境的状态\n",
      "\n",
      "在标准的 Cartpole 环境中，系统的状态由以下四个变量描述：\n",
      "\n",
      "1. **小车的位置 (x)**：小车在轨道上的位置。\n",
      "2. **小车的速度 (x')**：小车在水平方向上的速度。\n",
      "3. **杆子的角度 (θ)**：杆子与垂直线的夹角。\n",
      "4. **杆子的角速度 (θ')**：杆子旋转的速度。\n",
      "\n",
      "这些状态变量可以通过与环境的交互得到，并用于指导强化学习算法进行决策。\n",
      "\n",
      "### 动作空间\n",
      "\n",
      "Cartpole 环境中有两个可用的动作：\n",
      "\n",
      "- **向左施加力**（-1）：将小车向左推动。\n",
      "- **向右施加力**（+1）：将小车向右推动。\n",
      "\n",
      "每个动作的效果会影响小车的速度、位置以及杆子的角度，进一步决定是否能够成功保持平衡。\n",
      "\n",
      "### 奖励函数\n",
      "\n",
      "在 Cartpole 环境中，奖励函数是基于杆子是否保持平衡以及小车是否仍在轨道内来设计的。具体而言：\n",
      "\n",
      "- 每一个时间步，小车保持杆子垂直位置会获得一个 +1 的奖励。\n",
      "- 如果杆子的角度超过某个阈值（ ±12°），或者小车越过轨道边界（例如超出 x = [-2.4, 2.4]），则任务失败，环境将返回一个负奖励并终止当前回合。\n",
      "\n",
      "因此，系统的目标是最大化奖励，这等同于尽可能长时间地维持杆子的平衡。\n",
      "\n",
      "## 强化学习中的 Cartpole 应用\n",
      "\n",
      "Cartpole 问题是强化学习中经典的“平衡控制”问题。它在学习算法的设计和评估中具有重要意义。通过不断优化策略，强化学习算法能够学会如何调整小车的运动，以便保持杆子的平衡。\n",
      "\n",
      "Cartpole 环境的简单性和清晰的目标使其成为验证强化学习算法性能的理想选择。常见的强化学习算法，如 Q-learning、深度 Q 网络（DQN）、策略梯度方法等，通常都以 Cartpole 为基准进行测试。\n",
      "\n",
      "## 实现\n",
      "\n",
      "Cartpole 环境可以通过 OpenAI 提供的 Gym 库来实现。Gym 是一个流行的强化学习工具库，提供了包括 Cartpole 在内的多种经典强化学习环境。以下是使用 Gym 实现 Cartpole 环境的一个简单代码示例：\n",
      "\n",
      "```python\n",
      "import gym\n",
      "\n",
      "# 创建 Cartpole 环境\n",
      "env = gym.make('CartPole-v1')\n",
      "\n",
      "# 初始化环境\n",
      "state = env.reset()\n",
      "\n",
      "done = False\n",
      "while not done:\n",
      "    # 随机选择一个动作\n",
      "    action = env.action_space.sample()\n",
      "\n",
      "    # 执行动作，获取下一个状态、奖励和是否结束\n",
      "    next_state, reward, done, info = env.step(action)\n",
      "\n",
      "    # 渲染环境\n",
      "    env.render()\n",
      "\n",
      "# 关闭环境\n",
      "env.close()\n",
      "```\n",
      "\n",
      "## 总结\n",
      "\n",
      "Cartpole 是一个经典的强化学习问题，提供了一个简单但具有挑战性的控制任务。它常用于算法的验证与基准测试。通过不断优化控制策略，强化学习算法可以学会如何在给定的约束条件下稳定地平衡一个物理系统。\n",
      "\n",
      "\n",
      "文件名: p_iteration.py\n",
      "文件路径: p_iteration.py\n",
      "文件类型: .py\n",
      "---\n",
      "import gym\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import font_manager\n",
      "\n",
      "# 创建 CartPole 环境\n",
      "env = gym.make('CartPole-v1')\n",
      "\n",
      "# 定义状态离散化的区间\n",
      "cart_position_bins = np.linspace(-4.8, 4.8, 5)\n",
      "cart_velocity_bins = np.linspace(-0.5, 0.5, 9)\n",
      "pole_angle_bins = np.linspace(-0.20944 * 2, 0.20944 * 2, 5)  # 大约等于 ±12度 (弧度)\n",
      "pole_velocity_bins = np.linspace(-np.radians(50), np.radians(50), 9)\n",
      "\n",
      "def discretize_state(state):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
      "    state_discrete = [\n",
      "        np.digitize(cart_position, cart_position_bins),\n",
      "        np.digitize(cart_velocity, cart_velocity_bins),\n",
      "        np.digitize(pole_angle, pole_angle_bins),\n",
      "        np.digitize(pole_velocity, pole_velocity_bins)\n",
      "    ]\n",
      "    return tuple(state_discrete)\n",
      "\n",
      "\n",
      "# 初始化参数\n",
      "state_space_size = (len(cart_position_bins) + 1, len(cart_velocity_bins) + 1,\n",
      "                    len(pole_angle_bins) + 1, len(pole_velocity_bins) + 1)\n",
      "action_space_size = env.action_space.n\n",
      "\n",
      "# 初始化值函数表 V 和策略 π\n",
      "V = np.zeros(state_space_size)\n",
      "policy = np.zeros(state_space_size, dtype=int)  # 动作是 0 或 1\n",
      "\n",
      "gamma = 1  # 折扣因子\n",
      "\n",
      "R = np.zeros(state_space_size + (action_space_size,))\n",
      "Q = np.zeros(state_space_size + (action_space_size,))\n",
      "returns = np.zeros(state_space_size + (action_space_size,))\n",
      "state_action_count = np.zeros(state_space_size + (action_space_size,))\n",
      "p_total = np.zeros(state_space_size + (action_space_size,) + state_space_size)\n",
      "\n",
      "# 初始化状态转移字典\n",
      "transitions = {}\n",
      "\n",
      "def update_transitions(state, action, next_state):\n",
      "    if state not in transitions:\n",
      "        transitions[state] = {}\n",
      "    if action not in transitions[state]:\n",
      "        transitions[state][action] = {}\n",
      "    if next_state not in transitions[state][action]:\n",
      "        transitions[state][action][next_state] = 0\n",
      "    transitions[state][action][next_state] += 1\n",
      "\n",
      "def monte_carlo(num_episodes):\n",
      "    for episode in range(num_episodes):\n",
      "        state = discretize_state(env.reset()[0])\n",
      "        done = False\n",
      "        episode_data = []\n",
      "\n",
      "        while not done:\n",
      "            action = env.action_space.sample()\n",
      "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
      "            next_state = discretize_state(next_observation)\n",
      "            episode_data.append((state, action, reward))\n",
      "            update_transitions(state, action, next_state)\n",
      "            state = next_state\n",
      "            done = terminated or truncated\n",
      "        for state, action, reward in reversed(episode_data):\n",
      "            returns[state][action] += reward\n",
      "            state_action_count[state][action] += 1\n",
      "            # 计算每个状态-动作对的即时回报\n",
      "            R[state][action] = returns[state][action] / state_action_count[state][action]  # 更新 R 值\n",
      "        if episode % 100 == 0:\n",
      "            print(episode)\n",
      "\n",
      "    for state in transitions.keys():\n",
      "        for action in transitions[state].keys():\n",
      "            for next_state in transitions[state][action].keys():\n",
      "                p_total[state][action][next_state] = transitions[state][action][next_state] / state_action_count[state][action]\n",
      "\n",
      "# 策略迭代算法\n",
      "def policy_iteration(max_iterations):\n",
      "    for iteration in range(max_iterations + 1):\n",
      "        # 根据此次策略，更新状态价值\n",
      "        for state in transitions.keys():\n",
      "            action = policy[state]\n",
      "            if action not in transitions[state]:\n",
      "                continue\n",
      "            V[state] = R[state][action]\n",
      "            for next_state in transitions[state][action].keys():\n",
      "                if next_state not in transitions[state][action]:\n",
      "                    continue\n",
      "                V[state] += gamma * V[next_state] * p_total[state][action][next_state]\n",
      "        # 进行策略迭代，得到下一次策略\n",
      "        for state in transitions.keys():\n",
      "            action_values = []\n",
      "            for action in sorted(transitions[state].keys()):\n",
      "                Q[state][action] = R[state][action]\n",
      "                for next_state in transitions[state][action].keys():\n",
      "                    Q[state][action] += p_total[state][action][next_state] * (gamma * V[next_state])\n",
      "                action_values.append((Q[state][action], action))\n",
      "            m = 0\n",
      "            a = 0\n",
      "            for q, action in action_values:\n",
      "                if q > m:\n",
      "                    a = action\n",
      "                    m = q\n",
      "            policy[state] = a\n",
      "        if iteration % 100 == 0:\n",
      "            print(iteration)\n",
      "\n",
      "# 使用学习到的策略进行模拟测试\n",
      "def run_policy(env, episodes=100):\n",
      "    x = []\n",
      "    y = []\n",
      "    for episode in range(episodes):\n",
      "\n",
      "        observation, _ = env.reset()  # 获取观测状态\n",
      "        state = discretize_state(observation)\n",
      "        done = False\n",
      "        total_reward = 0\n",
      "        while not done:\n",
      "            env.render()\n",
      "            action = policy[state]\n",
      "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
      "            state = discretize_state(next_state)\n",
      "            total_reward += reward\n",
      "            done = terminated or truncated\n",
      "        print(f\"Episode {episode + 1}: Total reward = {total_reward}\")\n",
      "        x.append(episode + 1)\n",
      "        if (total_reward > 200):\n",
      "            total_reward = 200\n",
      "        y.append(total_reward)\n",
      "    # 设置字体，选择一个支持中文的字体\n",
      "    font_path = 'C:/Windows/Fonts/msyh.ttc'  # 替换为你的字体路径\n",
      "    font_prop = font_manager.FontProperties(fname=font_path)\n",
      "    plt.plot(x, y)\n",
      "    plt.xlabel('测试次数', fontproperties=font_prop)\n",
      "    plt.ylabel('总奖励（总步数）', fontproperties=font_prop)\n",
      "    plt.title('奖励与测试次数关系图', fontproperties=font_prop)\n",
      "    plt.savefig('policy.png')  # 保存为 PNG 格式\n",
      "    plt.show()  # Add this line to display the plot\n",
      "\n",
      "def main():\n",
      "    num = 100000\n",
      "    monte_carlo(num)\n",
      "    print(\"monte finish\")\n",
      "    max_iterations = 1000  # 最大迭代次数\n",
      "    policy_iteration(max_iterations)\n",
      "    # 运行测试，使用学习到的策略\n",
      "    run_policy(env)\n",
      "\n",
      "\n",
      "main()\n",
      "\n",
      "\n",
      "文件名: QLearning.py\n",
      "文件路径: QLearning.py\n",
      "文件类型: .py\n",
      "---\n",
      "## 目前最优：行为策略：随机策略，目标策略：贪心策略，学习率 = 1/(访问次数+0.5)，gamma=1\n",
      "import gym\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import font_manager\n",
      "\n",
      "# 创建 CartPole 环境\n",
      "env = gym.make('CartPole-v1')\n",
      "\n",
      "# 定义状态离散化的区间\n",
      "cart_position_bins = np.linspace(-4.8, 4.8, 5)\n",
      "cart_velocity_bins = np.linspace(-0.5, 0.5, 9)\n",
      "pole_angle_bins = np.linspace(-0.20944 * 2, 0.20944 * 2, 5)\n",
      "pole_velocity_bins = np.linspace(-np.radians(50), np.radians(50), 9)\n",
      "\n",
      "\n",
      "def discretize_state(state):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
      "    state_discrete = [\n",
      "        np.digitize(cart_position, cart_position_bins),\n",
      "        np.digitize(cart_velocity, cart_velocity_bins),\n",
      "        np.digitize(pole_angle, pole_angle_bins),\n",
      "        np.digitize(pole_velocity, pole_velocity_bins)\n",
      "    ]\n",
      "    return tuple(state_discrete)\n",
      "\n",
      "\n",
      "# 初始化参数\n",
      "state_space_size = (len(cart_position_bins) + 1, len(cart_velocity_bins) + 1,\n",
      "                    len(pole_angle_bins) + 1, len(pole_velocity_bins) + 1)\n",
      "action_space_size = env.action_space.n\n",
      "\n",
      "# 初始化 Q 值和计数\n",
      "Q = np.zeros(state_space_size + (action_space_size,))\n",
      "returns = np.zeros(state_space_size + (action_space_size,))\n",
      "state_action_count = np.zeros(state_space_size + (action_space_size,))\n",
      "\n",
      "# 参数\n",
      "gamma = 1  # 折扣因子\n",
      "epsilon = 0.8\n",
      "min_epsilon = 0.001\n",
      "decay_rate = 0.9999\n",
      "\n",
      "def qlearning_train(num_episodes):\n",
      "    global epsilon\n",
      "    for episode in range(num_episodes):\n",
      "        state = discretize_state(env.reset()[0])\n",
      "        done = False\n",
      "\n",
      "        while not done:\n",
      "            action = np.argmax(Q[state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
      "            # action = env.action_space.sample()\n",
      "            # action = np.argmax(Q[state])\n",
      "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
      "\n",
      "            next_state = discretize_state(next_observation)\n",
      "            # next_action = np.argmax(Q[next_state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
      "            # next_action = env.action_space.sample()\n",
      "            next_action = np.argmax(Q[next_state])\n",
      "\n",
      "            state_action_count[state][action] += 1\n",
      "            # Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
      "            alpha = 1.0 / (state_action_count[state][action] + 0.5)\n",
      "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
      "\n",
      "            state = next_state\n",
      "            done = terminated or truncated\n",
      "\n",
      "        # epsilon = max(min_epsilon, epsilon * decay_rate)\n",
      "        if episode % 100 == 0:\n",
      "            print(episode)\n",
      "\n",
      "\n",
      "def run_policy(episodes=100):\n",
      "    m = 500\n",
      "    x = []\n",
      "    y = []\n",
      "    for episode in range(episodes):\n",
      "        state = discretize_state(env.reset()[0])\n",
      "        done = False\n",
      "        total_reward = 0\n",
      "        while not done:\n",
      "            env.render()\n",
      "            action = np.argmax(Q[state])\n",
      "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
      "            state = discretize_state(next_state)\n",
      "            total_reward += reward\n",
      "            done = terminated or truncated\n",
      "        print(f\"Episode {episode + 1}: Total reward = {total_reward}\")\n",
      "        m = min(m, total_reward)\n",
      "        x.append(episode + 1)\n",
      "        y.append(total_reward)\n",
      "    print(m)\n",
      "    # 设置字体，选择一个支持中文的字体\n",
      "    font_path = 'C:/Windows/Fonts/msyh.ttc'  # 替换为你的字体路径\n",
      "    font_prop = font_manager.FontProperties(fname=font_path)\n",
      "    plt.plot(x, y)\n",
      "    # plt.xlabel('测试次数', fontproperties=font_prop)\n",
      "    # plt.ylabel('总奖励（总步数）', fontproperties=font_prop)\n",
      "    # plt.title('奖励与测试次数关系图', fontproperties=font_prop)\n",
      "    # plt.savefig('Sarsa.png')  # 保存为 PNG 格式\n",
      "    plt.show()  # Add this line to display the plot\n",
      "\n",
      "\n",
      "def main():\n",
      "    episodes_Q = 100000\n",
      "    qlearning_train(episodes_Q)\n",
      "    run_policy()\n",
      "    env.close()\n",
      "\n",
      "\n",
      "main()\n",
      "\n",
      "\n",
      "\n",
      "文件名: Sarsa.py\n",
      "文件路径: Sarsa.py\n",
      "文件类型: .py\n",
      "---\n",
      "## 目前最佳：随机策略，学习率=1/(访问次数+1)，gamma=1\n",
      "import gym\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import font_manager\n",
      "\n",
      "# 创建 CartPole 环境\n",
      "env = gym.make('CartPole-v1')\n",
      "\n",
      "# 定义状态离散化的区间\n",
      "cart_position_bins = np.linspace(-4.8, 4.8, 5)\n",
      "cart_velocity_bins = np.linspace(-0.5, 0.5, 9)\n",
      "pole_angle_bins = np.linspace(-0.20944 * 2, 0.20944 * 2, 5)\n",
      "pole_velocity_bins = np.linspace(-np.radians(50), np.radians(50), 9)\n",
      "\n",
      "\n",
      "def discretize_state(state):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
      "    state_discrete = [\n",
      "        np.digitize(cart_position, cart_position_bins),\n",
      "        np.digitize(cart_velocity, cart_velocity_bins),\n",
      "        np.digitize(pole_angle, pole_angle_bins),\n",
      "        np.digitize(pole_velocity, pole_velocity_bins)\n",
      "    ]\n",
      "    return tuple(state_discrete)\n",
      "\n",
      "\n",
      "# 初始化参数\n",
      "state_space_size = (len(cart_position_bins) + 1, len(cart_velocity_bins) + 1,\n",
      "                    len(pole_angle_bins) + 1, len(pole_velocity_bins) + 1)\n",
      "action_space_size = env.action_space.n\n",
      "\n",
      "# 初始化 Q 值和计数\n",
      "Q = np.zeros(state_space_size + (action_space_size,))\n",
      "returns = np.zeros(state_space_size + (action_space_size,))\n",
      "state_action_count = np.zeros(state_space_size + (action_space_size,))\n",
      "\n",
      "# 参数\n",
      "gamma = 1  # 折扣因子\n",
      "epsilon = 1\n",
      "min_epsilon = 0.001\n",
      "decay_rate = 0.99\n",
      "\n",
      "def sarsa_train(num_episodes):\n",
      "    global epsilon\n",
      "    for episode in range(num_episodes):\n",
      "        state = discretize_state(env.reset()[0])\n",
      "        done = False\n",
      "\n",
      "        while not done:\n",
      "\n",
      "            # action = np.argmax(Q[state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
      "            action = env.action_space.sample()\n",
      "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
      "\n",
      "            next_state = discretize_state(next_observation)\n",
      "            # next_action = np.argmax(Q[next_state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
      "            next_action = env.action_space.sample()\n",
      "\n",
      "            state_action_count[state][action] += 1\n",
      "            # Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
      "            alpha = 1.0 / (state_action_count[state][action] + 0.5)\n",
      "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
      "\n",
      "            state = next_state\n",
      "            done = terminated or truncated\n",
      "\n",
      "        epsilon = max(min_epsilon, epsilon * decay_rate)\n",
      "        if episode % 100 == 0:\n",
      "            print(episode)\n",
      "\n",
      "\n",
      "def run_policy(episodes=100):\n",
      "    x = []\n",
      "    y = []\n",
      "    for episode in range(episodes):\n",
      "        observation, _ = env.reset()  # 获取观测状态\n",
      "        state = discretize_state(observation)\n",
      "        done = False\n",
      "        total_reward = 0\n",
      "        while not done:\n",
      "            env.render()\n",
      "            action = np.argmax(Q[state])\n",
      "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
      "            state = discretize_state(next_state)\n",
      "            total_reward += reward\n",
      "            done = terminated or truncated\n",
      "        print(f\"Episode {episode + 1}: Total reward = {total_reward}\")\n",
      "        x.append(episode + 1)\n",
      "        y.append(total_reward)\n",
      "    # 设置字体，选择一个支持中文的字体\n",
      "    font_path = 'C:/Windows/Fonts/msyh.ttc'  # 替换为你的字体路径\n",
      "    font_prop = font_manager.FontProperties(fname=font_path)\n",
      "    plt.plot(x, y)\n",
      "    # plt.xlabel('测试次数', fontproperties=font_prop)\n",
      "    # plt.ylabel('总奖励（总步数）', fontproperties=font_prop)\n",
      "    # plt.title('奖励与测试次数关系图', fontproperties=font_prop)\n",
      "    # plt.savefig('Sarsa.png')  # 保存为 PNG 格式\n",
      "    plt.show()  # Add this line to display the plot\n",
      "\n",
      "\n",
      "def main():\n",
      "    episodes_Q = 100000\n",
      "    sarsa_train(episodes_Q)\n",
      "    run_policy()\n",
      "    env.close()\n",
      "\n",
      "\n",
      "main()\n",
      "\n",
      "\n",
      "\n",
      "文件名: v_iteration.py\n",
      "文件路径: v_iteration.py\n",
      "文件类型: .py\n",
      "---\n",
      "import gym\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import font_manager\n",
      "\n",
      "# 创建 CartPole 环境\n",
      "env = gym.make('CartPole-v1')\n",
      "\n",
      "# 定义状态离散化的区间\n",
      "cart_position_bins = np.linspace(-4.8, 4.8, 5)\n",
      "cart_velocity_bins = np.linspace(-0.5, 0.5, 9)\n",
      "pole_angle_bins = np.linspace(-0.20944 * 2, 0.20944 * 2, 5)  # 大约等于 ±12度 (弧度)\n",
      "pole_velocity_bins = np.linspace(-np.radians(50), np.radians(50), 9)\n",
      "\n",
      "def discretize_state(state):\n",
      "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
      "    state_discrete = [\n",
      "        np.digitize(cart_position, cart_position_bins),\n",
      "        np.digitize(cart_velocity, cart_velocity_bins),\n",
      "        np.digitize(pole_angle, pole_angle_bins),\n",
      "        np.digitize(pole_velocity, pole_velocity_bins)\n",
      "    ]\n",
      "    return tuple(state_discrete)\n",
      "\n",
      "\n",
      "# 初始化参数\n",
      "state_space_size = (len(cart_position_bins) + 1, len(cart_velocity_bins) + 1,\n",
      "                    len(pole_angle_bins) + 1, len(pole_velocity_bins) + 1)\n",
      "action_space_size = env.action_space.n\n",
      "\n",
      "# 初始化值函数表 V 和策略 π\n",
      "V = np.zeros(state_space_size)\n",
      "policy = np.zeros(state_space_size, dtype=int)  # 动作是 0 或 1\n",
      "\n",
      "# 值迭代算法的参数\n",
      "gamma = 1  # 折扣因子\n",
      "\n",
      "R = np.zeros(state_space_size + (action_space_size,))\n",
      "returns = np.zeros(state_space_size + (action_space_size,))\n",
      "state_action_count = np.zeros(state_space_size + (action_space_size,))\n",
      "\n",
      "state_action_state = np.zeros(state_space_size + (action_space_size,) + state_space_size)\n",
      "p_total = np.zeros(state_space_size + (action_space_size,) + state_space_size)\n",
      "\n",
      "# 初始化状态转移字典\n",
      "transitions = {}\n",
      "\n",
      "\n",
      "def update_transitions(state, action, next_state):\n",
      "    if state not in transitions:\n",
      "        transitions[state] = {}\n",
      "    if action not in transitions[state]:\n",
      "        transitions[state][action] = {}\n",
      "    if next_state not in transitions[state][action]:\n",
      "        transitions[state][action][next_state] = 0\n",
      "    transitions[state][action][next_state] += 1\n",
      "\n",
      "def monte_carlo(num_episodes):\n",
      "    for episode in range(num_episodes):\n",
      "        state = discretize_state(env.reset()[0])\n",
      "        done = False\n",
      "        episode_data = []\n",
      "\n",
      "        while not done:\n",
      "            # action = choose_policy(state)\n",
      "            action = env.action_space.sample()\n",
      "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
      "            next_state = discretize_state(next_observation)\n",
      "            episode_data.append((state, action, reward))\n",
      "            update_transitions(state, action, next_state)\n",
      "            state = next_state\n",
      "            done = terminated or truncated\n",
      "        for state, action, reward in reversed(episode_data):\n",
      "            returns[state][action] += reward\n",
      "            state_action_count[state][action] += 1\n",
      "            # 计算每个状态-动作对的即时回报\n",
      "            R[state][action] = returns[state][action] / state_action_count[state][action]  # 更新 R 值\n",
      "        if episode % 100 == 0:\n",
      "            print(episode)\n",
      "\n",
      "    for state in transitions.keys():\n",
      "        for action in transitions[state].keys():\n",
      "            for next_state in transitions[state][action].keys():\n",
      "                p_total[state][action][next_state] = transitions[state][action][next_state] / state_action_count[state][action]\n",
      "\n",
      "\n",
      "# 值迭代算法\n",
      "def value_iteration(max_iterations):\n",
      "    for iteration in range(max_iterations + 1):\n",
      "        for state in transitions.keys():\n",
      "            action_values = []\n",
      "            for action in sorted(transitions[state].keys()):\n",
      "                value = R[state][action]\n",
      "                for next_state in transitions[state][action].keys():\n",
      "                    value += p_total[state][action][next_state] * (gamma * V[next_state])\n",
      "                action_values.append((value, action))\n",
      "            m = 0\n",
      "            a = 0\n",
      "            for q, action in action_values:\n",
      "                if q > m:\n",
      "                    m = q\n",
      "                    a = action\n",
      "            # 更新值函数\n",
      "            V[state] = m\n",
      "            # 最后更新策略\n",
      "            if iteration == max_iterations:\n",
      "                policy[state] = a\n",
      "        if iteration % 100 == 0:\n",
      "            print(iteration)\n",
      "\n",
      "\n",
      "# 使用学习到的策略进行模拟测试\n",
      "def run_policy(env, episodes=100):\n",
      "    x = []\n",
      "    y = []\n",
      "    for episode in range(episodes):\n",
      "\n",
      "        observation, _ = env.reset()  # 获取观测状态\n",
      "        state = discretize_state(observation)\n",
      "        done = False\n",
      "        total_reward = 0\n",
      "        while not done:\n",
      "            env.render()\n",
      "            action = policy[state]\n",
      "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
      "            state = discretize_state(next_state)\n",
      "            total_reward += reward\n",
      "            done = terminated or truncated\n",
      "        print(f\"Episode {episode + 1}: Total reward = {total_reward}\")\n",
      "        x.append(episode + 1)\n",
      "        y.append(total_reward)\n",
      "    # 设置字体，选择一个支持中文的字体\n",
      "    font_path = 'C:/Windows/Fonts/msyh.ttc'  # 替换为你的字体路径\n",
      "    font_prop = font_manager.FontProperties(fname=font_path)\n",
      "    plt.plot(x, y)\n",
      "    plt.xlabel('测试次数', fontproperties=font_prop)\n",
      "    plt.ylabel('总奖励（总步数）', fontproperties=font_prop)\n",
      "    plt.title('奖励与测试次数关系图', fontproperties=font_prop)\n",
      "    plt.savefig('total_reward_plot.png')  # 保存为 PNG 格式\n",
      "    plt.show()  # Add this line to display the plot\n",
      "\n",
      "\n",
      "def main():\n",
      "    num = 100000\n",
      "    monte_carlo(num)\n",
      "    print(\"monte finish\")\n",
      "    max_iterations = 1000  # 最大迭代次数\n",
      "    value_iteration(max_iterations)\n",
      "    # 运行测试，使用学习到的策略\n",
      "    run_policy(env)\n",
      "\n",
      "\n",
      "main()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(i.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d5435cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED    \n",
      "deepseek-r1:latest          0a8c26691023    4.7 GB    3 weeks ago    \n",
      "nomic-embed-text:latest     0a109f422b47    274 MB    4 weeks ago    \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    4 weeks ago    \n",
      "qwen2.5-coder:latest        2b0496514337    4.7 GB    4 weeks ago    \n",
      "qwen2.5:latest              845dbda0ea48    4.7 GB    4 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "014e862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. First clean up any existing ChromaDB installations\n",
    "# %pip uninstall -y chromadb\n",
    "# %pip uninstall -y protobuf\n",
    "\n",
    "# # 2. Install specific versions known to work together\n",
    "# %pip install -q protobuf==3.20.3\n",
    "# %pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "# %pip install -q langchain-ollama\n",
    "\n",
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ed9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Try creating the vector database\n",
    "# vector_db = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "#     collection_name=\"local-rag\"\n",
    "# )\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings_path = 'BAAI/bge-large-zh-v1.5'\n",
    "# embeddings_path = \"/home/ubuntu/embedding_models/bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_path,\n",
    "    model_kwargs={\n",
    "        'device': device,\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,\n",
    "        'batch_size': 32\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "# local_model = \"llama3.2\"\n",
    "local_model = \"deepseek-r1\"\n",
    "# local_model = \"qwen2.5\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"你是一个 AI 语言模型助手。你的任务是为用户的问题生成五个不同的变体版本，\n",
    "    用于从向量数据库中检索相关文档。通过生成多个不同角度的问题，你的目标是帮助用户克服\n",
    "    基于距离相似度搜索的一些局限性。请提供这些替代性问题，每个问题用换行符分隔。\n",
    "    原始问题: {question}\"\"\",\n",
    ")\n",
    "# retriever = MultiQueryRetriever.from_llm(\n",
    "#     vector_db.as_retriever(), \n",
    "#     llm,\n",
    "#     prompt=QUERY_PROMPT\n",
    "# )\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "\n",
    "template = \"\"\"回答问题基于以下文档:\n",
    "{context}\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3e6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "嗯，我现在要解决的是使用QLearning算法来解决CartPole问题，并且给出一个可以直接运行的Python代码。我对CartPole问题有一定的了解，它是一个经典的控制任务，涉及到小车和杆子的状态，目标是让杆子保持垂直。\n",
       "\n",
       "首先，我需要理解CartPole的基本结构和状态空间。根据问题描述，状态由四个变量组成：小车的位置、速度、杆子的角度以及杆子的角速度。这四个状态变量将被编码为一个四维向量，作为Q表格中的键。\n",
       "\n",
       "接下来是动作空间。CartPole有两个动作，向左施加力（-1）和向右施加力（+1）。每个动作会改变环境的状态，影响未来的奖励。\n",
       "\n",
       "QLearning的基本原理是通过经验更新来逐步逼近最优策略。我需要设计一个Q表格，其中每一行对应一个状态，每一列对应一个动作，存储的是该状态下采取该动作的预期总回报。初始化时，可以将所有Q值设为0，表示对各个状态-动作组合的初始期望都不清楚。\n",
       "\n",
       "在选择动作的时候，我需要采用一种策略来决定是在探索还是利用。通常会使用ε-greedy策略，即以概率ε选择随机的动作，以概率1-ε选择当前最优的动作。这样可以平衡探索和利用之间的关系。\n",
       "\n",
       "训练过程将包括多次 episode，每次 episode 从初始状态开始，按照当前策略选择动作，执行动作后转移到下一个状态，并根据奖励更新Q值。更新公式通常是 Q[state, action] = Q[state, action] + α * (reward + γ * max(Q[next_state]) - Q[state, action])，其中α是学习率，γ是折扣因子。\n",
       "\n",
       "关于参数的选择， CartPole 的状态空间比较大，所以需要对状态进行离散化处理。这意味着将连续的状态值分割成有限的区间，并用整数来表示这些区间。例如，小车的位置可能被分为几个区间，每个区间对应一个离散的状态点。\n",
       "\n",
       "在编码实现时，首先需要对各个状态变量进行归一化和离散化。这包括小车位置、速度、杆子角度和杆子角速度。由于这些值的范围不同，可能需要分别处理，并确定每个维度上的分割点。\n",
       "\n",
       "接下来是初始化Q表格。考虑到状态被离散化后变成有限的状态数目，Q表可以是一个字典或二维列表。在Python中使用字典比较灵活，但为了高效访问，可以用数组结构或者字典的元组键。\n",
       "\n",
       "训练循环中，逐步迭代，更新策略和Q值，直到达到预设的终止条件，如最大训练次数或达到稳定的奖励值。\n",
       "\n",
       "最后是测试阶段，一旦模型训练完成后，需要在没有贪心策略的情况下运行，看看能否稳定地完成任务。或者可以使用最终 learned 的Q表来选择动作，执行整个 episode 并记录结果。\n",
       "\n",
       "现在，我需要将这些步骤整合成完整的Python代码，并确保每一步都能正确实现。特别是离散化状态和训练过程中的更新逻辑容易出错，需要仔细检查。\n",
       "</think>\n",
       "\n",
       "以下是使用 Q-Learning 算法解决 CartPole 问题的完整 Python 代码：\n",
       "\n",
       "```python\n",
       "import gym\n",
       "import numpy as np\n",
       "from collections import defaultdict\n",
       "\n",
       "def main():\n",
       "    # 定义 CartPole 环境和参数\n",
       "    env = gym.make('CartPole-v1')\n",
       "    \n",
       "    # 离散化状态空间，每个维度分割为若干区间\n",
       "    state_dim = [20, 20, 20, 20]  # 小车位置、速度、杆子角度、角速度的分段数\n",
       "    \n",
       "    # 定义各状态变量的范围\n",
       "    cart_pos_low = -4.8\n",
       "    cart_pos_high = 4.8\n",
       "    cart_vel_low = -3.14  # 原本应该是对称的，改为-2.4？或者其他值？\n",
       "    cart_vel_high = 3.14\n",
       "    \n",
       "    pole_angle_low = -0.5\n",
       "    pole_angle_high = 0.5  # 即±12度（因为 CartPole 的最大摆角是大约 ±12 度）\n",
       "    pole_angvel_low = -8.72  # 原本的±4.0？\n",
       "    pole_angvel_high = 8.72\n",
       "    \n",
       "    # 离散化函数\n",
       "    def state_to_index(state):\n",
       "        pos, vel, angle, angvel = state\n",
       "        \n",
       "        idx0 = np.digitize(pos, np.linspace(cart_pos_low, cart_pos_high, state_dim[0]))\n",
       "        idx1 = np.digitize(vel, np.linspace(cart_vel_low, cart_vel_high, state_dim[1]))\n",
       "        idx2 = np.digitize(angle, np.linspace(pole_angle_low, pole_angle_high, state_dim[2]))\n",
       "        idx3 = np.digitize(angvel, np.linspace(-8.72, 8.72, state_dim[3]))\n",
       "        \n",
       "        return (idx0, idx1, idx2, idx3)\n",
       "    \n",
       "    # 初始化 Q 表格\n",
       "    Q = defaultdict(lambda: [0.0] * 2)  # 每个状态有两个可能的动作\n",
       "    \n",
       "    # 超参数\n",
       "    alpha = 0.1  # 学习率\n",
       "    gamma = 0.99  # 折扣因子\n",
       "    epsilon = 0.1  # 探索率\n",
       "    num_episodes = 2000\n",
       "    max_steps_per_episode = 1000\n",
       "    \n",
       "    best_score = float('-inf')\n",
       "    \n",
       "    for episode in range(num_episodes):\n",
       "        state = env.reset()\n",
       "        score = 0\n",
       "        \n",
       "        if (episode + 1) % 50 == 0:\n",
       "            print(f\"Episode {episode + 1}/{num_episodes}\")\n",
       "        \n",
       "        while True:\n",
       "            # 根据当前策略选择动作（epsilon-greedy）\n",
       "            if np.random.random() < epsilon:\n",
       "                action = np.random.choice([0, 1])\n",
       "            else:\n",
       "                state_key = tuple(state_to_index(state))\n",
       "                current_q = Q[state_key]\n",
       "                action = np.argmax(current_q + 1e-6)  # 加一个小的数避免全零\n",
       "            \n",
       "            next_state, reward, done, _ = env.step(action)\n",
       "            score += reward\n",
       "            \n",
       "            # 离散化下一个状态\n",
       "            next_state_key = tuple(state_to_index(next_state))\n",
       "            \n",
       "            if done:\n",
       "                gamma = 0.9  # 终止状态的奖励处理\n",
       "                \n",
       "            # 更新 Q 表格\n",
       "            current_q = Q[state_key]\n",
       "            next_max_q = np.max(Q[next_state_key])\n",
       "            \n",
       "            old_q = current_q[action]\n",
       "            new_q = old_q + alpha * (reward + gamma * next_max_q - old_q)\n",
       "            Q[state_key][action] = new_q\n",
       "            \n",
       "            state = next_state\n",
       "            \n",
       "            if done or score >= 100:  # 如果完成任务或累积奖励达到100\n",
       "                break\n",
       "        \n",
       "        if score > best_score:\n",
       "            best_score = score\n",
       "            print(f\"Episode {episode + 1}: Best score {best_score}\")\n",
       "    \n",
       "    # 最终测试\n",
       "    test_state = env.reset()\n",
       "    while True:\n",
       "        state_key = tuple(state_to_index(test_state))\n",
       "        action = np.argmax(Q[state_key])  # 取最优动作\n",
       "        \n",
       "        next_test_state, reward, done, _ = env.step(action)\n",
       "        \n",
       "        if done:\n",
       "            break\n",
       "        \n",
       "        test_state = next_test_state\n",
       "    \n",
       "    print(\"完成任务！\")\n",
       "    \n",
       "if __name__ == '__main__':\n",
       "    main()\n",
       "```\n",
       "\n",
       "### 解释\n",
       "\n",
       "1. **环境初始化**：\n",
       "   - 使用 gym 库加载 CartPole 环境。\n",
       "   - 定义 Cart 和杆子的状态空间，将每个状态变量（位置、速度、角度和角速度）分别离散化为若干区间。\n",
       "\n",
       "2. **离散化函数**：\n",
       "   - 将连续的状态值转换为离散的索引，以便在 Q 表格中查找。\n",
       "\n",
       "3. **Q 表格初始化**：\n",
       "   - 使用字典存储每个状态-动作组合的 Q 值。键是状态索引元组，值是一个包含两个元素的列表（对应两个可能的动作）。\n",
       "\n",
       "4. **超参数设置**：\n",
       "   - 学习率 (alpha)、折扣因子 (gamma) 和探索率 (epsilon) 是训练过程中的重要参数。\n",
       "\n",
       "5. **训练循环**：\n",
       "   - 迭代一定次数，每个 episode 从初始状态开始。\n",
       "   - 使用 ε-greedy 策略选择动作，平衡探索和利用。\n",
       "   - 执行动作后，根据奖励更新 Q 值。\n",
       "\n",
       "6. **测试阶段**：\n",
       "   - 训练完成后，在使用 learned 的 Q 表格选择最优动作的情况下，执行整个 CartPole 任务，验证模型是否成功完成。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "s = chain.invoke(\"请给出用QLearning算法解决CartPole问题的完整，可以直接运行的Python代码\")\n",
    "# s = chain.invoke(\"what is the happiest country in the world?\")\n",
    "display(Markdown(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
